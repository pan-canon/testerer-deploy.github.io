<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Face Filter with MediaPipe</title>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_mesh"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>
  <style>
    body { margin: 0; display: flex; justify-content: center; align-items: center; height: 100vh; background-color: #000; }
    video { transform: scaleX(-1); width: 100%; max-width: 640px; }
    canvas { position: absolute; top: 0; left: 0; pointer-events: none; }
  </style>
</head>
<body>

<video id="video" autoplay></video>
<canvas id="canvas"></canvas>

<script>
  // Настройки камеры
  const videoElement = document.getElementById('video');
  const canvasElement = document.getElementById('canvas');
  const ctx = canvasElement.getContext('2d');
  const videoWidth = 640;
  const videoHeight = 480;

  // Устанавливаем камеру
  async function setupCamera() {
    const stream = await navigator.mediaDevices.getUserMedia({
      video: { width: videoWidth, height: videoHeight }
    });
    videoElement.srcObject = stream;
    await new Promise(resolve => {
      videoElement.onloadedmetadata = () => { resolve(videoElement); };
    });
  }

  // Инициализация MediaPipe FaceMesh
  const faceMesh = new FaceMesh.FaceMesh({
    locateFaces: true,
    minDetectionConfidence: 0.5,
    minTrackingConfidence: 0.5
  });

  // Запуск камеры и обработка
  async function detectFace() {
    await setupCamera();
    videoElement.play();
    canvasElement.width = videoWidth;
    canvasElement.height = videoHeight;

    function drawFaceLandmarks() {
      ctx.drawImage(videoElement, 0, 0, videoWidth, videoHeight);
      faceMesh.send({ image: videoElement }).then(results => {
        if (results.multiFaceLandmarks) {
          results.multiFaceLandmarks.forEach(landmarks => {
            // Рисуем точки на лице
            ctx.strokeStyle = '#FF0000';
            ctx.lineWidth = 2;
            landmarks.forEach(point => {
              ctx.beginPath();
              ctx.arc(point.x * videoWidth, point.y * videoHeight, 2, 0, 2 * Math.PI);
              ctx.fill();
            });

            // Здесь можно добавить фильтр, например, выделить глаза
            applyFilterToEyes(landmarks);
          });
        }
        requestAnimationFrame(drawFaceLandmarks);
      });
    }

    drawFaceLandmarks();
  }

  // Пример фильтра для выделения глаз
  function applyFilterToEyes(landmarks) {
    const leftEye = landmarks[33];  // Координаты левого глаза
    const rightEye = landmarks[133]; // Координаты правого глаза
    const eyeRadius = Math.abs(leftEye.x - rightEye.x) * videoWidth / 2;

    // Нарисуем круг вокруг глаз
    ctx.beginPath();
    ctx.arc(
      (leftEye.x + rightEye.x) * videoWidth / 2,
      (leftEye.y + rightEye.y) * videoHeight / 2,
      eyeRadius,
      0, 2 * Math.PI
    );
    ctx.lineWidth = 5;
    ctx.strokeStyle = 'rgba(0, 255, 0, 0.7)';
    ctx.stroke();
  }

  // Запускаем процесс
  detectFace();

</script>

</body>
</html>
