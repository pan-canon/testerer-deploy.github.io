<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Face-api.js Example</title>
  <script src="js/libs/face-api.js"></script>
</head>
<body>
  <h1>Face-api.js Example</h1>
  <button id="capture">Capture Selfie</button>
  <button id="compare">Compare with Camera</button>
  <video id="video" width="640" height="480" autoplay></video>
  <canvas id="canvas" style="display: none;"></canvas>

  <script>
    let videoElement = document.getElementById("video");
    let captureButton = document.getElementById("capture");
    let compareButton = document.getElementById("compare");
    let selfieImage = null; // Для хранения селфи
    let selfieDescriptor = null; // Для хранения дескриптора лица из селфи

    // Инициализация камеры
    async function setupCamera() {
      console.log("Setting up camera...");
      const stream = await navigator.mediaDevices.getUserMedia({ video: true });
      videoElement.srcObject = stream;
      await new Promise(resolve => videoElement.onplaying = resolve);
      console.log("Camera ready");
    }

    // Загружаем модели
    async function loadModels() {
      console.log("Loading models...");
      try {
        await Promise.all([
          faceapi.nets.ssdMobilenetv1.loadFromUri('./models'),
          faceapi.nets.faceLandmark68Net.loadFromUri('./models'),
          faceapi.nets.faceRecognitionNet.loadFromUri('./models')
        ]);
        console.log("Models loaded");
      } catch (error) {
        console.log("Error loading models:", error);
      }
    }

    // Запуск инициализации камеры и моделей
    async function init() {
      await setupCamera();
      await loadModels();
      console.log("Models loaded and camera set up, ready to capture!");
    }

    // Селфи: захват кадра с камеры и сохранение в формате base64
    captureButton.addEventListener("click", async () => {
      console.log("Capturing selfie...");
      const canvas = document.createElement('canvas');
      canvas.width = videoElement.videoWidth;
      canvas.height = videoElement.videoHeight;
      const ctx = canvas.getContext('2d');
      ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);
      selfieImage = canvas.toDataURL();
      console.log("Selfie captured:", selfieImage);

      // Проверяем, обнаружено ли лицо на селфи
      const detections = await detectFace(selfieImage);
      if (detections.length > 0) {
        selfieDescriptor = detections[0].descriptor;
        console.log("Selfie descriptor captured");
      } else {
        console.log("No face detected in selfie");
      }
    });

    // Сравнение селфи с текущим кадром
    compareButton.addEventListener("click", async () => {
      if (!selfieDescriptor) {
        console.log("No selfie captured to compare with.");
        return;
      }

      console.log("Comparing selfie with live camera feed...");

      const liveDetections = await detectFace(videoElement);
      if (liveDetections.length > 0) {
        const liveDescriptor = liveDetections[0].descriptor;

        // Сравниваем дескрипторы
        const distance = faceapi.euclideanDistance(selfieDescriptor, liveDescriptor);
        console.log("Face similarity:", distance);

        if (distance < 0.6) { // Снижение порога для совпадений
          console.log("Faces match!");
        } else {
          console.log("Faces do not match.");
        }
      } else {
        console.log("No face detected in the live camera feed.");
      }
    });

    // Функция для обнаружения лица и получения дескрипторов
    async function detectFace(imageData) {
      const img = new Image();
      img.src = imageData;
      await new Promise(resolve => img.onload = resolve);
      const detections = await faceapi.detectAllFaces(img)
        .withFaceLandmarks()
        .withFaceDescriptors();
      return detections;
    }

    // Запуск инициализации
    init();
  </script>
</body>
</html>
