<!DOCTYPE html>
<html lang="ru">
<head>
  <meta charset="UTF-8">
  <title>Сегментация + композитинг: изображение справа от человека</title>
  <!-- Подключаем TensorFlow.js и BodyPix -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@latest"></script>
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/body-pix@latest"></script>
  <style>
    body { margin: 0; overflow: hidden; }
    #container {
      position: relative;
      width: 640px;
      height: 480px;
      background: #000;
      margin: auto;
      top: 50px;
    }
    /* Видео занимает задний план */
    #videoElement {
      position: absolute;
      top: 0;
      left: 0;
      width: 640px;
      height: 480px;
      z-index: 1;
      object-fit: cover;
    }
    /* Основной canvas для финального композитинга */
    #canvasOverlay {
      position: absolute;
      top: 0;
      left: 0;
      width: 640px;
      height: 480px;
      z-index: 3;
    }
    /* Дополнительное информационное окно (опционально) */
    #overlayInfo {
      position: absolute;
      top: 10px;
      left: 10px;
      z-index: 4;
      color: #fff;
      background: rgba(0,0,0,0.5);
      padding: 5px 10px;
      border-radius: 4px;
      font-size: 16px;
      pointer-events: none;
    }
  </style>
</head>
<body>
  <div id="container">
    <video id="videoElement" autoplay playsinline muted></video>
    <canvas id="canvasOverlay"></canvas>
    <div id="overlayInfo"></div>
  </div>
  <button id="startButton">Запустить камеру</button>
  <button id="stopButton">Остановить камеру</button>

  <script>
    // Элементы DOM
    const videoElement = document.getElementById("videoElement");
    const canvasOverlay = document.getElementById("canvasOverlay");
    const ctx = canvasOverlay.getContext("2d");
    const overlayInfo = document.getElementById("overlayInfo");

    // Offscreen canvas для дополнительного изображения
    const offCanvas = document.createElement("canvas");
    const offCtx = offCanvas.getContext("2d");

    let stream = null;
    let segmentationModel = null;
    let isRunning = false;

    // Легкая конфигурация BodyPix
    const bodyPixConfig = {
      architecture: 'MobileNetV1',
      outputStride: 16,
      multiplier: 0.75,
      quantBytes: 2
    };

    // Загружаем дополнительное изображение (например, декоративный элемент)
    const extraImg = new Image();
    extraImg.src = "images/static-image.webp"; // корректный путь к изображению
    extraImg.onload = () => {
      console.log("Дополнительное изображение загружено");
    };

    // Запуск камеры
    async function startCamera() {
      try {
        stream = await navigator.mediaDevices.getUserMedia({ video: { facingMode: "user" } });
        videoElement.srcObject = stream;
        videoElement.onloadedmetadata = () => {
          videoElement.play();
          canvasOverlay.width = videoElement.videoWidth;
          canvasOverlay.height = videoElement.videoHeight;
          offCanvas.width = videoElement.videoWidth;
          offCanvas.height = videoElement.videoHeight;
        };
      } catch (err) {
        console.error("Ошибка доступа к камере:", err);
      }
    }

    // Загрузка модели BodyPix
    async function loadModel() {
      console.log("Загрузка модели BodyPix...");
      segmentationModel = await bodyPix.load(bodyPixConfig);
      console.log("Модель BodyPix загружена");
    }

    // Вычисление bounding box по бинарной маске (данные segmentation.data)
    function getBoundingBox(segmentationData, width, height) {
      let minX = width, minY = height, maxX = 0, maxY = 0;
      for (let y = 0; y < height; y++) {
        for (let x = 0; x < width; x++) {
          const index = y * width + x;
          if (segmentationData[index] === 1) {
            if (x < minX) minX = x;
            if (x > maxX) maxX = x;
            if (y < minY) minY = y;
            if (y > maxY) maxY = y;
          }
        }
      }
      return { minX, minY, maxX, maxY };
    }

    // Основной цикл композитинга
    async function compositeFrame() {
      if (!isRunning) return;
      // Получаем сегментацию человека
      const segmentation = await segmentationModel.segmentPerson(videoElement, {
        flipHorizontal: false,
        internalResolution: 'medium',
        segmentationThreshold: 0.7
      });
      // Маска в виде массива (1 – принадлежит человеку)
      const maskData = segmentation.data;
      const segWidth = segmentation.width;
      const segHeight = segmentation.height;

      // Вычисляем bounding box для человека в координатах маски
      const bbox = getBoundingBox(maskData, segWidth, segHeight);
      // Вычисляем коэффициенты масштабирования для перехода к координатам канваса
      const scaleX = canvasOverlay.width / segWidth;
      const scaleY = canvasOverlay.height / segHeight;
      // Переводим bounding box в координаты канваса
      const personX = bbox.minX * scaleX;
      const personY = bbox.minY * scaleY;
      const personWidth = (bbox.maxX - bbox.minX) * scaleX;
      const personHeight = (bbox.maxY - bbox.minY) * scaleY;
      overlayInfo.innerHTML = `Человек: (${Math.round(personX)}, ${Math.round(personY)}) - (${Math.round(personX + personWidth)}, ${Math.round(personY + personHeight)})`;

      // Подготовка offscreen canvas для дополнительного изображения:
      offCtx.clearRect(0, 0, offCanvas.width, offCanvas.height);
      offCtx.drawImage(extraImg, 0, 0, offCanvas.width, offCanvas.height);

      // Создаем маску из сегментации с помощью bodyPix.toMask
      const maskImage = bodyPix.toMask(segmentation, {r:255, g:255, b:255, a:255}, {r:0, g:0, b:0, a:0});
      // Применяем маску: режим destination-out вырезает область, где маска белая
      offCtx.globalCompositeOperation = "destination-out";
      offCtx.putImageData(new ImageData(maskImage.data, maskImage.width, maskImage.height), 0, 0);
      offCtx.globalCompositeOperation = "source-over";
      
      // Композитинг на основном canvas:
      ctx.clearRect(0, 0, canvasOverlay.width, canvasOverlay.height);
      // Сначала рисуем видео
      ctx.drawImage(videoElement, 0, 0, canvasOverlay.width, canvasOverlay.height);
      
      // Теперь разместим дополнительное изображение справа от человека, так чтобы его вертикальный размер равнялся personHeight.
      // Вычисляем желаемые размеры с учетом сохранения соотношения сторон:
      const extraAspect = extraImg.width / extraImg.height;
      const extraHeight = personHeight;
      const extraWidth = extraHeight * extraAspect;
      // Выбираем позицию: x – сразу после правого края bounding box, y – равен personY (чтобы изображение шло от головы до ног)
      const extraX = personX + personWidth;
      const extraY = personY;

      // Опционально: можно задать clip, чтобы гарантировать, что изображение не выйдет в область человека.
      ctx.save();
      // Область, куда будем рисовать extraImage (слева ограничивается extraX)
      ctx.beginPath();
      ctx.rect(extraX, extraY, extraWidth, extraHeight);
      ctx.clip();
      // Рисуем обработанное дополнительное изображение с offCanvas, масштабируя его до размера extraWidth x extraHeight
      ctx.drawImage(offCanvas, 0, 0, offCanvas.width, offCanvas.height, extraX, extraY, extraWidth, extraHeight);
      ctx.restore();

      requestAnimationFrame(compositeFrame);
    }

    async function startComposite() {
      isRunning = true;
      await loadModel();
      compositeFrame();
    }

    function stop() {
      isRunning = false;
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }
      ctx.clearRect(0, 0, canvasOverlay.width, canvasOverlay.height);
      overlayInfo.innerHTML = "";
    }

    document.getElementById("startButton").addEventListener("click", async () => {
      await startCamera();
      startComposite();
    });
    document.getElementById("stopButton").addEventListener("click", stop);
  </script>
</body>
</html>